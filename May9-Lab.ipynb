{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93323831-fe1b-478d-8b88-241f199277a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63a96624-b431-463a-a8a4-8e28910cd7da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# NLP Performance Evaluation Metrics\n",
    "\n",
    "### BLEU Score:\n",
    "- Measure of the precision of n-grams in the model output, against the reference text, that is human-generated.\n",
    "- Initially designed for machine translations tasks, but it's been adopted widely across several NLP tasks.\n",
    "- BLEU stands for Bilingual Evaluation Understudy.\n",
    "\n",
    "BLEU can be thought of as a grammar checker for machines. \n",
    "- It compares what a machine wrote to what a human wrote, and checks how many of the same words and short phrases appear in both.\n",
    "- It's like grading a student's essay by how many of the same word patterns it uses compared to a perfect answer.\n",
    "- It’s very strict — it gives higher scores when the machine uses exactly the same words in the same order as a human would.\n",
    "- Commonly used to evaluate machine translations or summaries.\n",
    "- What it measures: Precision — how many of the words the machine used were actually right (according to the human example).\n",
    "\n",
    "###  ROUGE Score: \n",
    "- Specifically more focused on recall.\n",
    "- Compares overlapping units like n-grams, words sequences, and word-pairs, in both generated text and the reference text.\n",
    "- ROUGE scores commonly used for specific NLP tasks like text summarization.\n",
    "- ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation.\n",
    "\n",
    "ROUGE is more like a reading comprehension teacher. \n",
    "- It checks how much of the important stuff from the human-written version the machine remembered and included.\n",
    "- It’s more forgiving than BLEU — it doesn’t require the words to be in the exact same order.\n",
    "- Often used for testing how good a machine-generated summary is.\n",
    "- What it measures: Recall — how much of the important content from the human version was captured by the machine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b4321-613f-44ec-89a3-fb384d45aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed libraries\n",
    "import evaluate\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed813cf-0349-41bf-ae9b-a9a7c7b1f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentences\n",
    "# Candidate is the LLM generated version\n",
    "reference = [\"the cat is on the mat\"]\n",
    "candidate = [\"the cat is on mat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a1995-f217-485a-8f47-1bbe7d6a1d31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BLEU Score Calculation\n",
    "bleu = corpus_bleu(candidate, [reference])\n",
    "print(f\"BLEU Score: {bleu.score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b24ec-7cd2-4e7f-a47e-3d00ecd3969e",
   "metadata": {},
   "source": [
    "## Explanation of Results\n",
    "\n",
    "The score means that the machine-generated sentence (candidate) matches the human-written reference fairly well, but not perfectly.\n",
    "- BLEU scores range from 0 to 100 (sometimes shown as 0.0 to 1.0 depending on the library).\n",
    "- A higher score means a closer match between the candidate and the reference in terms of exact word choices and word order.\n",
    "- A score of 57.89 means the overlap is moderately strong, but some words or phrases don’t exactly match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8677398d-937e-4875-abbb-761399f73324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE Score Calculation\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference[0], candidate[0])\n",
    "print(f\"ROUGE-1: {scores['rouge1']}\")\n",
    "print(f\"ROUGE-L: {scores['rougeL']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8254a2a2-599a-41fb-86eb-29d76c6bef26",
   "metadata": {},
   "source": [
    "## Explanation of Results\n",
    "\n",
    "##### ROUGE-1\n",
    "\n",
    "Measures overlap of single words (unigrams) between the reference and the candidate.\n",
    "- Precision = 1.0 → Every word in the candidate was found in the reference.\n",
    "- Recall = 0.833... → 83.33% of the words in the reference were captured in the candidate.\n",
    "- F1-score = 0.909... → The harmonic mean of precision and recall; gives a balanced view of performance.\n",
    "- Interpretation: The candidate used all correct words, but missed one or two from the reference.\n",
    "\n",
    "##### ROUGE-L\n",
    "\n",
    "Measures longest common subsequence (LCS) which is the longest series of words that appear in both the reference and candidate in the same order, though not necessarily contiguously.\n",
    "- Same scores here imply the LCS was very good\n",
    "- All words in the candidate were correct and in order, but the candidate missed a small portion of the original meaning/content.\n",
    "- Interpretation: The structure and ordering of the candidate closely matched the human version, missing only a little content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c43a24e-8d0d-4c1f-9861-5127a1f1332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative code using the sacreblue library\n",
    "\n",
    "from sacrebleu import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Example sentences\n",
    "reference = [\"the cat is on the mat\"]\n",
    "candidate = [\"the cat is on mat\"]\n",
    "\n",
    "# BLEU Score Calculation\n",
    "bleu = corpus_bleu(candidate, [reference])\n",
    "print(f\"BLEU Score: {bleu.score}\")\n",
    "\n",
    "# ROUGE Score Calculation\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference[0], candidate[0])\n",
    "print(f\"ROUGE-1: {scores['rouge1']}\")\n",
    "print(f\"ROUGE-L: {scores['rougeL']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c50d5-52d1-41ca-b744-b53c80104545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab",
   "language": "python",
   "name": "may9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
